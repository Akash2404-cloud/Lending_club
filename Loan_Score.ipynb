{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb808f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName('Loan Score') \\\n",
    ".config('spark.shuffle.useOldFetchProtocol' , 'true') \\\n",
    ".config('spark.sql.warehouse.dir' , 'user/itv006879/warehouse') \\\n",
    ".enableHiveSupport() \\\n",
    ".master('yarn') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541aab4",
   "metadata": {},
   "source": [
    "## Associating points to grades for usage in Loan score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbe59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.unacceptable_rated_pts' , 0)\n",
    "spark.conf.set('spark.sql.very_bad_rated_pts' , 100)\n",
    "spark.conf.set('spark.sql.bad_rated_pts' , 250)\n",
    "spark.conf.set('spark.sql.good_rated_pts' , 500)\n",
    "spark.conf.set('spark.sql.very_good_rated_pts' , 650)\n",
    "spark.conf.set('spark.sql.excellent_rated_pts' , 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432710d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.unacceptable_grade_pts' , 750)\n",
    "spark.conf.set('spark.sql.very_bad_grade_pts' , 1000)\n",
    "spark.conf.set('spark.sql.bad_rated_grade_pts' , 1500)\n",
    "spark.conf.set('spark.sql.good_grade_pts' , 2000)\n",
    "spark.conf.set('spark.sql.very_good_grade_pts' , 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628ea20",
   "metadata": {},
   "source": [
    "# The Tables required are:-\n",
    "\n",
    "### customers_new\n",
    "\n",
    "### loans\n",
    "\n",
    "### loans_repayment\n",
    "\n",
    "### loans_defaulters_delinq_new\n",
    "\n",
    "### loans_defaulters_detail_records_enq_new\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7273c40",
   "metadata": {},
   "source": [
    "# 1. Payment history points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc525b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_cust_df =spark.read \\\n",
    ".format('csv') \\\n",
    ".option('header' , 'true')\\\n",
    ".load('user/itv006879/bad_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe735e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data_cust_df.createOrReplaceTempView('bad_data_customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_df = spark.sql(''' select c.member_id ,\n",
    "case\n",
    "   when p.last_payment_amount < (c.monthly_installments * 0.5) then ${spark.sql.very_bad_rated_pts}\n",
    "   when p.last_payment_amount > (c.monthly_installments * 0.5) and p.last_payment_amount < c.monthly_installment then ${spark.sql.bad_rated_pts}\n",
    "   when p.last_payment_amount = (c.monthly_installments * 0.5) then ${spark.sql.good_rated_pts}\n",
    "   when p.last_payment_amount > (c.monthly_installments ) and p.last_payment < (c.monthly_installment * 1.50) then ${spark.sql.very_good_rated_pts}\n",
    "   when p.last_payment_amount > (c.monthly_installments * 1.50)  then ${spark.sql.excellent_rated_pts}\n",
    "end as last_payment_pts ,\n",
    "case\n",
    "   when p.total_payment_received >= (c.funded_payment * 0.5) then ${spark.sql.very_good_rated_pts}\n",
    "   when p.total_payment_received < (c.funded_payment * 0.5) and p.total_payment_received > 0 then ${spark.sql.good_rated_pts}\n",
    "   when p.total_payment_received = 0 or p.total_payment_received is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as total_payment_received_pts\n",
    "from lending_club_proj_6879.loan_repayments p \n",
    "inner join loans c on c.loan_id = p.loan_id where not member_id in (select * from bad_data_customers)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_df.createOrReplaceTempView('ph_pts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af51464",
   "metadata": {},
   "source": [
    "# 2.Loan defaulter history points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74482f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldh_ph_df = spark.sql(''' select p.*\n",
    "case\n",
    "when d.delinq_2yrs = 0 then ${spark.sql.excellent_rated_pts}\n",
    "when d.delinq_2yrs between 1 and 2 then ${spark.sql.bad_rated_pts}\n",
    "when d.delinq_2yrs between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "when d.delinq_2yrs > 5 or d.delinq_2yrs is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as delinq_pts ,\n",
    "case\n",
    "when l.pub_rec = 0 then ${spark.sql.excellent_rated_pts}\n",
    "when l.pub_rec between 1 and 2  then ${spark.sql.bad_rated_pts}\n",
    "when l.pub_rec between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "when l.pub_rec > 5 or d.delinq_2yrs is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as public_record_pts,\n",
    "case\n",
    "when l.pub_rec_bankruptcies = 0 then ${spark.sql.excellent_rated_pts}\n",
    "when l.pub_rec_bankruptcies between 1 and 2  then ${spark.sql.bad_rated_pts}\n",
    "when l.pub_rec_bankruptcies between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "when l.pub_rec_bankruptcies > 5 or d.delinq_2yrs is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as public_bankruptcies_pts,\n",
    "case\n",
    "when l.inq_last_6mths = 0 then ${spark.sql.excellent_rated_pts}\n",
    "when l.inq_last_6mths between 1 and 2  then ${spark.sql.bad_rated_pts}\n",
    "when l.inq_last_6mths between 3 and 5 then ${spark.sql.very_bad_rated_pts}\n",
    "when l.inq_last_6mths > 5 or d.delinq_2yrs is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as enq_pts\n",
    "from  lending_club_proj_6879.loans_defaulters_detail_records_enq_new l\n",
    "inner join lending_club_proj_6879.loans_defaulters_delinq_new d on d.member_id = l.member_id \n",
    "inner join ph_pts p on p.member_id = l.member where not l.member_id in (select * from bad_data_customers)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb00dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldh_ph_df.createOrReplaceTempView('ldh_ph_pts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d30c3",
   "metadata": {},
   "source": [
    "## 3. Financial Health points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_ldh_ph_df = spark.sql(''' select ldef.* ,\n",
    "case\n",
    "when lower(l.loan_status) is like '%fully paid%' then ${spark.sql.excellent_rated_pts}\n",
    "when lower(l.loan_status) is like '%current%' then ${spark.sql.good_rated_pts}\n",
    "when lower(l.loan_status) is like '%in grace period%' then ${spark.sql.bad_rated_pts}\n",
    "when lower(l.loan_status) is like '%late (16-30 days)%' or lower(loan_status) is like '%late (31 - 120 days)%' then ${spark.sql.very_bad_rated_pts}\n",
    "when lower(l.loan_status) is like '%charged off%' then ${spark.sql.unacceptable_rated_pts}\n",
    "end as loan_status_pts ,\n",
    "case\n",
    "when lower(a.home_ownership) like '%own'then ${spark.sql.excellent_rated_pts}\n",
    "when lower(a.home_ownership) is like '%rent%' then ${spark.sql.good_rated_pts}\n",
    "when lower(a.home_ownership) is like '%mortgage%' then ${spark.sql.bad_rated_pts}\n",
    "when lower(a.home_ownership) is like '%any%' or lower(loan_status) is null then ${spark.sql.unacceptable_rated_pts}\n",
    "end as home_pts\n",
    "case\n",
    "when l.funded_amount <= (a.total_high_credit_limit * 0.1) then ${spark.sql.excellent_rated_pts}\n",
    "when l.funded_amount > (a.total_high_credit_limit * 0.1) and l.funded_amount < (a.total_high_credit_limit * 0.3) then ${spark.sql.good_rated_pts}\n",
    "when l.funded_amount <= (a.total_high_credit_limit * 0.3) and l.funded_amount < (a.total_high_credit_limit * 0.5) then ${spark.sql.bad_rated_pts}\n",
    "when l.funded_amount <= (a.total_high_credit_limit * 0.5) and l.funded_amount < (a.total_high_credit_limit * 0.9) then ${spark.sql.very_bad_rated_pts}\n",
    "when l.funded_amount = a.total_high_credit_limit then ${spark.sql.unacceptable_rated_pts}\n",
    "end as funded_amt_pts\n",
    "case\n",
    "when a.grade = 'A' and a.sub_grade in ('A1','A2' , 'A3' , 'A4' , 'A5') then ${spark.sql.excellent_rated_pts}\n",
    "when a.grade = 'B' and a.sub_grade in ('B1','B2' , 'B3' , 'B4' , 'B5') then ${spark.sql.good_rated_pts}\n",
    "when a.grade = 'C' and a.sub_grade in ('C1','C2' , 'C3' , 'C4' , 'C5') then ${spark.sql.bad_rated_pts}\n",
    "when a.grade = 'D' and a.sub_grade in ('D1','D2' , 'D3' , 'D4' , 'D5') then ${spark.sql.very_bad_rated_pts}\n",
    "when a.grade = 'E' and a.sub_grade in ('E1','E2' , 'E3' , 'E4' , 'E5') then ${spark.sql.unacceptable_rated_pts}\n",
    "end as grade_pts\n",
    "from ldh_ph_pts ldef\n",
    "inner join lending_club_proj_6879.loans l on ldef.member_id = l.member_id \n",
    "inner join lending_club_proj_6879.customer_new a on a.member_id = ledf.member_id \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a94812",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh_ldh_ph_df.createOrReplaceTempView('fh_ldh_ph_pts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef316a6e",
   "metadata": {},
   "source": [
    "## Loan score calculation\n",
    "\n",
    "### payment_history = 20%\n",
    "### loan_default_history = 45%\n",
    "### financial health = 35%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dcf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_df = spark.sql('''\n",
    "select ((last_payment_pts + total_payment_received_pts) * 0.2 ) as payment_history_pts ,\n",
    "((delinq_pts + public_record_pts + public_bankruptcies_pts + enq_pts) * 0.45) as loan_default_history_pts,\n",
    "((loan_status_pts + home_pts + funded_amt_pts + grade_pts) * 0.35) as c\n",
    "from fh_ldh_ph_pts\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_df = loan_score_df.withColumn('Loan_total_score' , loan_score_df.payment_history_pts + loan_score_df.loan_default_history_pts + loan_score_df.loan_score_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_df.createOrReplaceTempView('final_loan_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff48722",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_eval = spark.sql('''\n",
    "select ls.* ,\n",
    "case\n",
    "when ls.Loan_total_score >= ${'spark.sql.very_good_grade_pts'} then A+\n",
    "when ls.Loan_total_score between ${'spark.sql.good_grade_pts'} and ${'spark.sql.very_good_grade_pts'}-1 then B\n",
    "when ls.Loan_total_score betwneen ${'spark.sql.bad_rated_grade_pts'} and  ${'spark.sql.good_grade_pts'}-1  then C\n",
    "when ls.Loan_total_score between ${'spark.sql.very_bad_grade_pts'}  and ${'spark.sql.bad_rated_grade_pts'}-1 then D\n",
    "when ls.Loan_total_score between ${'spark.sql.unacceptable_grade_pts'} and ${'spark.sql.very_bad_grade_pts'} -1 then E\n",
    "when ls.Loan_total_score < ${'spark.sql.unacceptable_grade_pts'} then F\n",
    "end as loan_grade\n",
    "from final_loan_score ls\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_eval.repartition(1).write \\\n",
    ".format('parquet') \\\n",
    ".option('header' , 'true') \\\n",
    ".mode('overwrite') \\\n",
    ".option('path' , '/user/itv006879/processed/') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bba20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
